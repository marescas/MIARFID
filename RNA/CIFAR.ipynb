{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CIFAR.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SD-gi4m2aZPh",
        "outputId": "43516259-501e-4cce-9a36-fb6c2400e446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers.normalization import BatchNormalization as BN\n",
        "from keras.layers import GaussianNoise as GN\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "from keras.callbacks import LearningRateScheduler as LRS\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import  matplotlib.pyplot as plt\n",
        "import math\n",
        "import numpy as np\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76wi3Oxiz327",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "class MixupGenerator():\n",
        "    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.batch_size = batch_size\n",
        "        self.alpha = alpha\n",
        "        self.shuffle = shuffle\n",
        "        self.sample_num = len(X_train)\n",
        "        self.datagen = datagen\n",
        "\n",
        "    def __call__(self):\n",
        "        while True:\n",
        "            indexes = self.__get_exploration_order()\n",
        "            itr_num = int(len(indexes) // (self.batch_size * 2))\n",
        "\n",
        "            for i in range(itr_num):\n",
        "                batch_ids = indexes[i * self.batch_size * 2:(i + 1) * self.batch_size * 2]\n",
        "                X, y = self.__data_generation(batch_ids)\n",
        "\n",
        "                yield X, y\n",
        "\n",
        "    def __get_exploration_order(self):\n",
        "        indexes = np.arange(self.sample_num)\n",
        "\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(indexes)\n",
        "\n",
        "        return indexes\n",
        "\n",
        "    def __data_generation(self, batch_ids):\n",
        "        _, h, w, c = self.X_train.shape\n",
        "        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n",
        "        X_l = l.reshape(self.batch_size, 1, 1, 1)\n",
        "        y_l = l.reshape(self.batch_size, 1)\n",
        "\n",
        "        X1 = self.X_train[batch_ids[:self.batch_size]]\n",
        "        X2 = self.X_train[batch_ids[self.batch_size:]]\n",
        "        X = X1 * X_l + X2 * (1 - X_l)\n",
        "\n",
        "        if self.datagen:\n",
        "            for i in range(self.batch_size):\n",
        "                X[i] = self.datagen.random_transform(X[i])\n",
        "                X[i] = self.datagen.standardize(X[i])\n",
        "\n",
        "        if isinstance(self.y_train, list):\n",
        "            y = []\n",
        "\n",
        "            for y_train_ in self.y_train:\n",
        "                y1 = y_train_[batch_ids[:self.batch_size]]\n",
        "                y2 = y_train_[batch_ids[self.batch_size:]]\n",
        "                y.append(y1 * y_l + y2 * (1 - y_l))\n",
        "        else:\n",
        "            y1 = self.y_train[batch_ids[:self.batch_size]]\n",
        "            y2 = self.y_train[batch_ids[self.batch_size:]]\n",
        "            y = y1 * y_l + y2 * (1 - y_l)\n",
        "\n",
        "        return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yisHdQ3zacaD",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 150\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5bb-H2dXagAK",
        "outputId": "9fc61d8c-b203-4207-916d-f2d1eb3158da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#### LOAD AND TRANSFORM\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "(50000, 32, 32, 3)\n",
            "(10000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TNvc70hUajtk",
        "colab": {}
      },
      "source": [
        "## DEFINE A DATA AUGMENTATION GENERATOR\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    rotation_range = 15,\n",
        "    horizontal_flip=True)\n",
        "\n",
        "###########################################################\n",
        "# Now this is necessary due to the feature normalization: #\n",
        "datagen.fit(x_train)\n",
        "training_generator = MixupGenerator(x_train, y_train, batch_size=batch_size, alpha=0.2, datagen=datagen)()\n",
        "\n",
        "###########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZzaxQiRatZH",
        "outputId": "d8fd3518-5df1-4862-97c1-20fba185f520",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## DEF A BLOCK CONV + BN + GN + MAXPOOL\n",
        "def CBGN(model,filters,ishape=0):\n",
        "    if (ishape!=0):\n",
        "        model.add(Conv2D(filters, (3, 3), padding='same',input_shape=ishape))\n",
        "    else:\n",
        "        model.add(Conv2D(filters, (3, 3), padding='same'))\n",
        "        \n",
        "    model.add(BN())\n",
        "    model.add(GN(0.15))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(Conv2D(filters, (3, 3), padding='same'))    \n",
        "    model.add(BN())\n",
        "    model.add(GN(0.15))\n",
        "    model.add(Activation('relu'))\n",
        "    \n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "  \n",
        "    return model\n",
        "\n",
        "## DEF NN TOPOLOGY  \n",
        "model = Sequential()\n",
        "\n",
        "model=CBGN(model,32,x_train.shape[1:])\n",
        "model=CBGN(model,64)\n",
        "model=CBGN(model,128)\n",
        "model=CBGN(model,256)\n",
        "model=CBGN(model,512)\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_1 (GaussianNo (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_2 (GaussianNo (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_3 (GaussianNo (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_4 (GaussianNo (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "gaussian_noise_6 (GaussianNo (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 4, 4, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_7 (GaussianNo (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 4, 4, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_8 (Batch (None, 4, 4, 256)         1024      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_8 (GaussianNo (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 4, 4, 256)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 2, 2, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "batch_normalization_9 (Batch (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_9 (GaussianNo (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 2, 2, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_10 (Batc (None, 2, 2, 512)         2048      \n",
            "_________________________________________________________________\n",
            "gaussian_noise_10 (GaussianN (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 1, 1, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "activation_11 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_12 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 4,987,946\n",
            "Trainable params: 4,983,978\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uSa3RqFPaWbn",
        "outputId": "7ff347f9-a71d-4484-d2ba-01b4d1b53b62",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "## OPTIM AND COMPILE\n",
        "opt = SGD(lr=0.1, decay=1e-6)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# DEFINE A LEARNING RATE SCHEDULER\n",
        "def scheduler(epoch):\n",
        "    if epoch < 50:\n",
        "        return .1\n",
        "    elif epoch < 75:\n",
        "        return 0.05\n",
        "    elif epoch < 100:\n",
        "        return 0.01\n",
        "    else:\n",
        "        return 0.001\n",
        "    \n",
        "def step_decay(epoch):\n",
        "    initial_lrate = 0.1\n",
        "    drop = 0.9\n",
        "    epochs_drop = 25.0\n",
        "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
        "    if lrate < 0.001:\n",
        "        lrate =0.001\n",
        "    return lrate\n",
        "\n",
        "set_lr = LRS(scheduler)\n",
        "\n",
        "\n",
        "## TRAINING with DA and LRA\n",
        "history=model.fit_generator(generator = datagen.flow(x_train,y_train,batch_size=batch_size),\n",
        "                            steps_per_epoch=len(x_train) / batch_size, \n",
        "                            epochs=epochs,\n",
        "                            validation_data=(x_test, y_test),\n",
        "                            callbacks=[set_lr],\n",
        "                            verbose=1)\n",
        "\n",
        "\n",
        "## TEST\n",
        "scores = model.evaluate(x_test, y_test, verbose=1) \n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1]) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "391/390 [==============================] - 31s 79ms/step - loss: 1.8702 - accuracy: 0.3199 - val_loss: 1.9788 - val_accuracy: 0.3113\n",
            "Epoch 2/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 1.4088 - accuracy: 0.4868 - val_loss: 1.6309 - val_accuracy: 0.4434\n",
            "Epoch 3/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 1.1966 - accuracy: 0.5716 - val_loss: 1.1159 - val_accuracy: 0.6118\n",
            "Epoch 4/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 1.0361 - accuracy: 0.6338 - val_loss: 1.0392 - val_accuracy: 0.6484\n",
            "Epoch 5/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.9349 - accuracy: 0.6690 - val_loss: 0.9443 - val_accuracy: 0.6636\n",
            "Epoch 6/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.8519 - accuracy: 0.7007 - val_loss: 1.1261 - val_accuracy: 0.6312\n",
            "Epoch 7/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.7936 - accuracy: 0.7239 - val_loss: 0.8918 - val_accuracy: 0.7051\n",
            "Epoch 8/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.7479 - accuracy: 0.7399 - val_loss: 0.8166 - val_accuracy: 0.7314\n",
            "Epoch 9/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.7101 - accuracy: 0.7523 - val_loss: 0.7149 - val_accuracy: 0.7579\n",
            "Epoch 10/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.6777 - accuracy: 0.7635 - val_loss: 0.8074 - val_accuracy: 0.7371\n",
            "Epoch 11/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.6484 - accuracy: 0.7737 - val_loss: 0.7648 - val_accuracy: 0.7601\n",
            "Epoch 12/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.6274 - accuracy: 0.7830 - val_loss: 0.7836 - val_accuracy: 0.7458\n",
            "Epoch 13/150\n",
            "391/390 [==============================] - 25s 65ms/step - loss: 0.6072 - accuracy: 0.7893 - val_loss: 0.9971 - val_accuracy: 0.6977\n",
            "Epoch 14/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.5875 - accuracy: 0.7945 - val_loss: 0.7678 - val_accuracy: 0.7615\n",
            "Epoch 15/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.5680 - accuracy: 0.8022 - val_loss: 0.6714 - val_accuracy: 0.7746\n",
            "Epoch 16/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.5532 - accuracy: 0.8082 - val_loss: 0.7335 - val_accuracy: 0.7704\n",
            "Epoch 17/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.5357 - accuracy: 0.8122 - val_loss: 0.6018 - val_accuracy: 0.7964\n",
            "Epoch 18/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.5200 - accuracy: 0.8182 - val_loss: 0.6734 - val_accuracy: 0.7763\n",
            "Epoch 19/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.5076 - accuracy: 0.8241 - val_loss: 0.6383 - val_accuracy: 0.7862\n",
            "Epoch 20/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4969 - accuracy: 0.8259 - val_loss: 0.5381 - val_accuracy: 0.8213\n",
            "Epoch 21/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4874 - accuracy: 0.8300 - val_loss: 0.6171 - val_accuracy: 0.7956\n",
            "Epoch 22/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4752 - accuracy: 0.8338 - val_loss: 0.6115 - val_accuracy: 0.8038\n",
            "Epoch 23/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4711 - accuracy: 0.8373 - val_loss: 0.6576 - val_accuracy: 0.7918\n",
            "Epoch 24/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.4538 - accuracy: 0.8433 - val_loss: 0.4828 - val_accuracy: 0.8387\n",
            "Epoch 25/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4464 - accuracy: 0.8441 - val_loss: 0.6092 - val_accuracy: 0.8103\n",
            "Epoch 26/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.4410 - accuracy: 0.8451 - val_loss: 0.4734 - val_accuracy: 0.8441\n",
            "Epoch 27/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4333 - accuracy: 0.8493 - val_loss: 0.4226 - val_accuracy: 0.8557\n",
            "Epoch 28/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4216 - accuracy: 0.8512 - val_loss: 0.4803 - val_accuracy: 0.8343\n",
            "Epoch 29/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4170 - accuracy: 0.8553 - val_loss: 0.5141 - val_accuracy: 0.8315\n",
            "Epoch 30/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.4119 - accuracy: 0.8559 - val_loss: 0.4711 - val_accuracy: 0.8433\n",
            "Epoch 31/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3960 - accuracy: 0.8623 - val_loss: 0.5153 - val_accuracy: 0.8346\n",
            "Epoch 32/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3941 - accuracy: 0.8630 - val_loss: 0.4297 - val_accuracy: 0.8549\n",
            "Epoch 33/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.3894 - accuracy: 0.8623 - val_loss: 0.6504 - val_accuracy: 0.8013\n",
            "Epoch 34/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3824 - accuracy: 0.8659 - val_loss: 0.5417 - val_accuracy: 0.8286\n",
            "Epoch 35/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3750 - accuracy: 0.8694 - val_loss: 0.5099 - val_accuracy: 0.8428\n",
            "Epoch 36/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.3736 - accuracy: 0.8696 - val_loss: 0.5050 - val_accuracy: 0.8391\n",
            "Epoch 37/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3677 - accuracy: 0.8698 - val_loss: 0.4162 - val_accuracy: 0.8591\n",
            "Epoch 38/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3585 - accuracy: 0.8744 - val_loss: 0.5462 - val_accuracy: 0.8292\n",
            "Epoch 39/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.3559 - accuracy: 0.8753 - val_loss: 0.4209 - val_accuracy: 0.8621\n",
            "Epoch 40/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3487 - accuracy: 0.8774 - val_loss: 0.6551 - val_accuracy: 0.8100\n",
            "Epoch 41/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3470 - accuracy: 0.8775 - val_loss: 0.4933 - val_accuracy: 0.8472\n",
            "Epoch 42/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.3402 - accuracy: 0.8803 - val_loss: 0.4261 - val_accuracy: 0.8621\n",
            "Epoch 43/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3347 - accuracy: 0.8821 - val_loss: 0.5841 - val_accuracy: 0.8219\n",
            "Epoch 44/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3278 - accuracy: 0.8860 - val_loss: 0.4580 - val_accuracy: 0.8591\n",
            "Epoch 45/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3240 - accuracy: 0.8852 - val_loss: 0.4371 - val_accuracy: 0.8570\n",
            "Epoch 46/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3213 - accuracy: 0.8865 - val_loss: 0.4058 - val_accuracy: 0.8712\n",
            "Epoch 47/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3214 - accuracy: 0.8867 - val_loss: 0.4213 - val_accuracy: 0.8641\n",
            "Epoch 48/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3187 - accuracy: 0.8886 - val_loss: 0.4617 - val_accuracy: 0.8554\n",
            "Epoch 49/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3060 - accuracy: 0.8931 - val_loss: 0.4171 - val_accuracy: 0.8701\n",
            "Epoch 50/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.3066 - accuracy: 0.8929 - val_loss: 0.4083 - val_accuracy: 0.8686\n",
            "Epoch 51/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.2732 - accuracy: 0.9031 - val_loss: 0.3805 - val_accuracy: 0.8770\n",
            "Epoch 52/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.2658 - accuracy: 0.9067 - val_loss: 0.3735 - val_accuracy: 0.8805\n",
            "Epoch 53/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.2568 - accuracy: 0.9093 - val_loss: 0.3832 - val_accuracy: 0.8803\n",
            "Epoch 54/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.2532 - accuracy: 0.9107 - val_loss: 0.3715 - val_accuracy: 0.8835\n",
            "Epoch 55/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.2479 - accuracy: 0.9118 - val_loss: 0.3773 - val_accuracy: 0.8836\n",
            "Epoch 56/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.2441 - accuracy: 0.9133 - val_loss: 0.3684 - val_accuracy: 0.8860\n",
            "Epoch 57/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2501 - accuracy: 0.9116 - val_loss: 0.3662 - val_accuracy: 0.8845\n",
            "Epoch 58/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2382 - accuracy: 0.9147 - val_loss: 0.3723 - val_accuracy: 0.8850\n",
            "Epoch 59/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2430 - accuracy: 0.9137 - val_loss: 0.3765 - val_accuracy: 0.8837\n",
            "Epoch 60/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2389 - accuracy: 0.9160 - val_loss: 0.3884 - val_accuracy: 0.8831\n",
            "Epoch 61/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2330 - accuracy: 0.9167 - val_loss: 0.3725 - val_accuracy: 0.8855\n",
            "Epoch 62/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2345 - accuracy: 0.9157 - val_loss: 0.4052 - val_accuracy: 0.8770\n",
            "Epoch 63/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2322 - accuracy: 0.9184 - val_loss: 0.3557 - val_accuracy: 0.8921\n",
            "Epoch 64/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2279 - accuracy: 0.9192 - val_loss: 0.3481 - val_accuracy: 0.8936\n",
            "Epoch 65/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2246 - accuracy: 0.9200 - val_loss: 0.3876 - val_accuracy: 0.8824\n",
            "Epoch 66/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2311 - accuracy: 0.9184 - val_loss: 0.3648 - val_accuracy: 0.8875\n",
            "Epoch 67/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2225 - accuracy: 0.9201 - val_loss: 0.3676 - val_accuracy: 0.8905\n",
            "Epoch 68/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2225 - accuracy: 0.9215 - val_loss: 0.3712 - val_accuracy: 0.8892\n",
            "Epoch 69/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2223 - accuracy: 0.9217 - val_loss: 0.3944 - val_accuracy: 0.8798\n",
            "Epoch 70/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2197 - accuracy: 0.9214 - val_loss: 0.3711 - val_accuracy: 0.8837\n",
            "Epoch 71/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2208 - accuracy: 0.9210 - val_loss: 0.3689 - val_accuracy: 0.8913\n",
            "Epoch 72/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2165 - accuracy: 0.9217 - val_loss: 0.3999 - val_accuracy: 0.8800\n",
            "Epoch 73/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2189 - accuracy: 0.9222 - val_loss: 0.4063 - val_accuracy: 0.8801\n",
            "Epoch 74/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2149 - accuracy: 0.9237 - val_loss: 0.3976 - val_accuracy: 0.8835\n",
            "Epoch 75/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.2131 - accuracy: 0.9246 - val_loss: 0.3936 - val_accuracy: 0.8818\n",
            "Epoch 76/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1915 - accuracy: 0.9324 - val_loss: 0.3568 - val_accuracy: 0.8920\n",
            "Epoch 77/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1840 - accuracy: 0.9345 - val_loss: 0.3382 - val_accuracy: 0.8977\n",
            "Epoch 78/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1822 - accuracy: 0.9355 - val_loss: 0.3453 - val_accuracy: 0.8968\n",
            "Epoch 79/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1802 - accuracy: 0.9354 - val_loss: 0.3404 - val_accuracy: 0.8987\n",
            "Epoch 80/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1755 - accuracy: 0.9376 - val_loss: 0.3369 - val_accuracy: 0.9005\n",
            "Epoch 81/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1757 - accuracy: 0.9363 - val_loss: 0.3438 - val_accuracy: 0.9018\n",
            "Epoch 82/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1743 - accuracy: 0.9382 - val_loss: 0.3544 - val_accuracy: 0.8974\n",
            "Epoch 83/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1730 - accuracy: 0.9384 - val_loss: 0.3442 - val_accuracy: 0.8976\n",
            "Epoch 84/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1732 - accuracy: 0.9373 - val_loss: 0.3396 - val_accuracy: 0.8992\n",
            "Epoch 85/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1681 - accuracy: 0.9396 - val_loss: 0.3435 - val_accuracy: 0.9019\n",
            "Epoch 86/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1708 - accuracy: 0.9387 - val_loss: 0.3449 - val_accuracy: 0.9001\n",
            "Epoch 87/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1721 - accuracy: 0.9379 - val_loss: 0.3405 - val_accuracy: 0.9007\n",
            "Epoch 88/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1694 - accuracy: 0.9398 - val_loss: 0.3375 - val_accuracy: 0.9010\n",
            "Epoch 89/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1685 - accuracy: 0.9401 - val_loss: 0.3320 - val_accuracy: 0.9039\n",
            "Epoch 90/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1651 - accuracy: 0.9423 - val_loss: 0.3443 - val_accuracy: 0.9015\n",
            "Epoch 91/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1692 - accuracy: 0.9396 - val_loss: 0.3451 - val_accuracy: 0.9028\n",
            "Epoch 92/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1655 - accuracy: 0.9406 - val_loss: 0.3475 - val_accuracy: 0.8998\n",
            "Epoch 93/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1663 - accuracy: 0.9411 - val_loss: 0.3490 - val_accuracy: 0.9010\n",
            "Epoch 94/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1615 - accuracy: 0.9420 - val_loss: 0.3470 - val_accuracy: 0.9006\n",
            "Epoch 95/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1686 - accuracy: 0.9406 - val_loss: 0.3577 - val_accuracy: 0.8967\n",
            "Epoch 96/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1630 - accuracy: 0.9428 - val_loss: 0.3472 - val_accuracy: 0.9010\n",
            "Epoch 97/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1658 - accuracy: 0.9401 - val_loss: 0.3390 - val_accuracy: 0.9031\n",
            "Epoch 98/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1607 - accuracy: 0.9430 - val_loss: 0.3493 - val_accuracy: 0.8986\n",
            "Epoch 99/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1627 - accuracy: 0.9417 - val_loss: 0.3866 - val_accuracy: 0.8881\n",
            "Epoch 100/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1640 - accuracy: 0.9414 - val_loss: 0.3475 - val_accuracy: 0.9007\n",
            "Epoch 101/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1574 - accuracy: 0.9436 - val_loss: 0.3427 - val_accuracy: 0.9026\n",
            "Epoch 102/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1550 - accuracy: 0.9440 - val_loss: 0.3409 - val_accuracy: 0.9026\n",
            "Epoch 103/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1540 - accuracy: 0.9457 - val_loss: 0.3421 - val_accuracy: 0.9017\n",
            "Epoch 104/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1571 - accuracy: 0.9447 - val_loss: 0.3419 - val_accuracy: 0.9019\n",
            "Epoch 105/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1539 - accuracy: 0.9453 - val_loss: 0.3396 - val_accuracy: 0.9032\n",
            "Epoch 106/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1543 - accuracy: 0.9445 - val_loss: 0.3452 - val_accuracy: 0.9015\n",
            "Epoch 107/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1536 - accuracy: 0.9464 - val_loss: 0.3453 - val_accuracy: 0.9018\n",
            "Epoch 108/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1530 - accuracy: 0.9452 - val_loss: 0.3439 - val_accuracy: 0.9016\n",
            "Epoch 109/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1523 - accuracy: 0.9462 - val_loss: 0.3467 - val_accuracy: 0.9013\n",
            "Epoch 110/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1519 - accuracy: 0.9460 - val_loss: 0.3464 - val_accuracy: 0.9017\n",
            "Epoch 111/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1551 - accuracy: 0.9435 - val_loss: 0.3470 - val_accuracy: 0.9020\n",
            "Epoch 112/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1556 - accuracy: 0.9442 - val_loss: 0.3455 - val_accuracy: 0.9027\n",
            "Epoch 113/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1539 - accuracy: 0.9448 - val_loss: 0.3433 - val_accuracy: 0.9035\n",
            "Epoch 114/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1563 - accuracy: 0.9448 - val_loss: 0.3454 - val_accuracy: 0.9024\n",
            "Epoch 115/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1558 - accuracy: 0.9455 - val_loss: 0.3449 - val_accuracy: 0.9019\n",
            "Epoch 116/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1499 - accuracy: 0.9460 - val_loss: 0.3438 - val_accuracy: 0.9022\n",
            "Epoch 117/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1558 - accuracy: 0.9443 - val_loss: 0.3462 - val_accuracy: 0.9017\n",
            "Epoch 118/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1556 - accuracy: 0.9443 - val_loss: 0.3426 - val_accuracy: 0.9028\n",
            "Epoch 119/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1486 - accuracy: 0.9479 - val_loss: 0.3433 - val_accuracy: 0.9022\n",
            "Epoch 120/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1532 - accuracy: 0.9453 - val_loss: 0.3433 - val_accuracy: 0.9018\n",
            "Epoch 121/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1479 - accuracy: 0.9475 - val_loss: 0.3454 - val_accuracy: 0.9020\n",
            "Epoch 122/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1519 - accuracy: 0.9461 - val_loss: 0.3416 - val_accuracy: 0.9026\n",
            "Epoch 123/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1513 - accuracy: 0.9460 - val_loss: 0.3442 - val_accuracy: 0.9025\n",
            "Epoch 124/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1509 - accuracy: 0.9457 - val_loss: 0.3422 - val_accuracy: 0.9027\n",
            "Epoch 125/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1515 - accuracy: 0.9459 - val_loss: 0.3436 - val_accuracy: 0.9025\n",
            "Epoch 126/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1551 - accuracy: 0.9462 - val_loss: 0.3436 - val_accuracy: 0.9018\n",
            "Epoch 127/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1498 - accuracy: 0.9465 - val_loss: 0.3435 - val_accuracy: 0.9022\n",
            "Epoch 128/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1511 - accuracy: 0.9461 - val_loss: 0.3450 - val_accuracy: 0.9027\n",
            "Epoch 129/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1537 - accuracy: 0.9459 - val_loss: 0.3441 - val_accuracy: 0.9016\n",
            "Epoch 130/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1535 - accuracy: 0.9450 - val_loss: 0.3450 - val_accuracy: 0.9023\n",
            "Epoch 131/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1496 - accuracy: 0.9470 - val_loss: 0.3445 - val_accuracy: 0.9028\n",
            "Epoch 132/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1500 - accuracy: 0.9465 - val_loss: 0.3432 - val_accuracy: 0.9023\n",
            "Epoch 133/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1514 - accuracy: 0.9459 - val_loss: 0.3488 - val_accuracy: 0.9027\n",
            "Epoch 134/150\n",
            "391/390 [==============================] - 25s 63ms/step - loss: 0.1558 - accuracy: 0.9448 - val_loss: 0.3456 - val_accuracy: 0.9033\n",
            "Epoch 135/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1541 - accuracy: 0.9445 - val_loss: 0.3435 - val_accuracy: 0.9026\n",
            "Epoch 136/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1517 - accuracy: 0.9456 - val_loss: 0.3444 - val_accuracy: 0.9032\n",
            "Epoch 137/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1471 - accuracy: 0.9467 - val_loss: 0.3455 - val_accuracy: 0.9034\n",
            "Epoch 138/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1508 - accuracy: 0.9464 - val_loss: 0.3437 - val_accuracy: 0.9029\n",
            "Epoch 139/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1492 - accuracy: 0.9468 - val_loss: 0.3443 - val_accuracy: 0.9022\n",
            "Epoch 140/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1508 - accuracy: 0.9455 - val_loss: 0.3429 - val_accuracy: 0.9025\n",
            "Epoch 141/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1526 - accuracy: 0.9455 - val_loss: 0.3450 - val_accuracy: 0.9026\n",
            "Epoch 142/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1501 - accuracy: 0.9459 - val_loss: 0.3449 - val_accuracy: 0.9027\n",
            "Epoch 143/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1511 - accuracy: 0.9467 - val_loss: 0.3434 - val_accuracy: 0.9025\n",
            "Epoch 144/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1494 - accuracy: 0.9457 - val_loss: 0.3420 - val_accuracy: 0.9026\n",
            "Epoch 145/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1498 - accuracy: 0.9461 - val_loss: 0.3431 - val_accuracy: 0.9016\n",
            "Epoch 146/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1501 - accuracy: 0.9463 - val_loss: 0.3418 - val_accuracy: 0.9020\n",
            "Epoch 147/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1491 - accuracy: 0.9462 - val_loss: 0.3423 - val_accuracy: 0.9020\n",
            "Epoch 148/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1505 - accuracy: 0.9459 - val_loss: 0.3439 - val_accuracy: 0.9022\n",
            "Epoch 149/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1474 - accuracy: 0.9479 - val_loss: 0.3442 - val_accuracy: 0.9026\n",
            "Epoch 150/150\n",
            "391/390 [==============================] - 25s 64ms/step - loss: 0.1461 - accuracy: 0.9475 - val_loss: 0.3432 - val_accuracy: 0.9028\n",
            "10000/10000 [==============================] - 3s 252us/step\n",
            "Test loss: 0.3432067557007074\n",
            "Test accuracy: 0.9028000235557556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HycOqww_z33M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def step_decay(epoch):\n",
        "    initial_lrate = 0.2\n",
        "    drop = 0.9\n",
        "    epochs_drop = 25.0\n",
        "    lrate = initial_lrate * math.pow(drop,math.floor((1+epoch)/epochs_drop))\n",
        "    if lrate < 0.001:\n",
        "        print(\"bajo\")\n",
        "        lrate =0.001\n",
        "    return lrate\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt-rlmVfz33P",
        "colab_type": "code",
        "outputId": "ffb69962-5102-4eb8-aa65-170f28e23b91",
        "colab": {}
      },
      "source": [
        "epochs= []\n",
        "lr = []\n",
        "for item in range(1,250):\n",
        "    epochs.append(item)\n",
        "    lr.append(step_decay(item))\n",
        "plt.plot(epochs,lr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x249a1238148>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4jxzMfez33S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import SVG\n",
        "from keras.utils import plot_model\n",
        "#Mostrar arquitectura de la red\n",
        "plot_model(model, to_file='model.png')\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCFW9dwdz33U",
        "colab_type": "code",
        "outputId": "35dd9fdd-a976-4bf2-8c3d-c588a7f3cd52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        }
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#Evolución del la funcion de perdida en train y test\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a5c1262dd61d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plE0DAm-z33X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}