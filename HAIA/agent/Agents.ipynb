{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar-v0\n",
    "\n",
    "## Q learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valores máximos observacion:  [0.6  0.07]\n",
      "valores mínimos observacion:  [-1.2  -0.07]\n",
      "posibles estados [19 15]\n",
      "posibles acciones 3\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "#Observaciones:\n",
    "# 0: posición\n",
    "# 1: velocidad\n",
    "print(\"valores máximos observacion: \",env.observation_space.high)\n",
    "print(\"valores mínimos observacion: \",env.observation_space.low)\n",
    "#discretizamos los posibles estados\n",
    "num_estados = (env.observation_space.high - env.observation_space.low) * np.array([10, 100]) \n",
    "num_estados = np.round(num_estados, 0).astype(int) + 1\n",
    "print(\"posibles estados\",num_estados)\n",
    "#acciones:\n",
    "# 0: izquierda\n",
    "# 1: nada\n",
    "# 2: derecha\n",
    "print(\"posibles acciones\",env.action_space.n)\n",
    "#Creamos la tabla Q de dimensiones estados * accioens\n",
    "Q = np.zeros((num_estados[0],num_estados[1],env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodios = 10000\n",
    "exploracion_vs_explotacion = 0.8\n",
    "alpha = 0.1\n",
    "gamma = 0.8\n",
    "for episodio in range(num_episodios):\n",
    "    terminal = False\n",
    "    #Obtenemos el estado inicial\n",
    "    estado = env.reset()\n",
    "    #discretizar estado\n",
    "    estado = (estado - env.observation_space.low) * np.array([10, 100]) \n",
    "    estado = np.round(estado, 0).astype(int) + 1\n",
    "    while not terminal:\n",
    "        if episodio > 9900:\n",
    "            env.render()\n",
    "       \n",
    "        #determinar la accion a realizar mediante compromiso exploración vs explotación epsilon-greedy\n",
    "        if np.random.random() < exploracion_vs_explotacion:\n",
    "            #explotación escogemos la mejor acción para un estado dado\n",
    "            accion = np.argmax(Q[estado[0],estado[1]])\n",
    "        else:\n",
    "            #exploracion escogemos una acción aleatoria\n",
    "            accion = np.random.randint(0,env.action_space.n)\n",
    "        #tomamos la accion escogida y observamos\n",
    "        estado2,recompensa,terminal,_ = env.step(accion)\n",
    "        #Discretizamos el nuevo estado\n",
    "        if estado2[0] ==0.5:\n",
    "            print(\"llego al final\")\n",
    "            alpha = 0.001\n",
    "            exploracion_vs_explotacion = 0.9\n",
    "        estado2 = (estado2 - env.observation_space.low) * np.array([10, 100]) \n",
    "        estado2 = np.round(estado2, 0).astype(int) + 1\n",
    "        #Actualizamos la tabla Q\n",
    "        Q[estado[0],estado[1],accion] += alpha * (recompensa + gamma * np.max(Q[estado2[0],estado2[1]]) - Q[estado[0],estado[1],accion] ) \n",
    "        #actualizamos el estado\n",
    "        estado = estado2.copy()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import random\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 64)                192       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 24)                1560      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 1,827\n",
      "Trainable params: 1,827\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=2, activation='relu'))\n",
    "model.add(Dense(24, activation='relu'))\n",
    "model.add(Dense(int(env.action_space.n), activation='linear'))\n",
    "model.compile(loss='mse',optimizer=Adam(lr=0.01))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "episodios = 100000\n",
    "exploracion_vs_explotacion = 0.8\n",
    "memoria = deque(maxlen = 2000)\n",
    "tamañoBatch = 32\n",
    "gamma = 0.9\n",
    "for episodio in range(episodios):\n",
    "    estado = env.reset()\n",
    "    terminal = False\n",
    "    score = 0\n",
    "    while not terminal:\n",
    "        #obtener una acción con el compromiso exploración explotacion\n",
    "        if np.random.random() < exploracion_vs_explotacion:\n",
    "            #explotación escogemos la mejor acción para un estado dado\n",
    "            accion = np.argmax(model.predict(np.reshape(estado,[1,2])))\n",
    "            #print(accion)\n",
    "        else:\n",
    "            #exploracion escogemos una acción aleatoria\n",
    "            accion = np.random.randint(0,env.action_space.n)\n",
    "\n",
    "\n",
    "        estado2,recompensa,terminal,_ = env.step(accion)\n",
    "        score+=recompensa\n",
    "        memoria.append((estado,accion,estado2,recompensa,terminal))\n",
    "        estado = estado2\n",
    "    #print(len(memoria))\n",
    "    #entrenamos con lo jugado\n",
    "    if tamañoBatch > len(memoria):\n",
    "        minibatch = random.sample(memoria,len(memoria))\n",
    "    else:\n",
    "        minibatch = random.sample(memoria,tamañoBatch)\n",
    "    for estado,accion,estado2,recompensa,terminal in minibatch:\n",
    "        target = recompensa\n",
    "        if not terminal:\n",
    "            target = recompensa + gamma* np.max(model.predict(np.reshape(estado2,[1,2])))\n",
    "        target_red = model.predict(np.reshape(estado,[1,2]))\n",
    "        target_red[0][accion] = target\n",
    "        model.fit(np.reshape(estado,[1,2]),target_red,epochs=1, verbose=0)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
